{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashi-Dwivedi1812/Rag-Query-Engine/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-eZBhPJ-MIv4",
        "outputId": "6b7af34a-f104-4c80-a3ad-65a154e8e038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m114.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain_community\n",
            "  Downloading langchain_community-0.3.31-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=0.3.78 (from langchain_community)\n",
            "  Downloading langchain_core-0.3.78-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting langchain<2.0.0,>=0.3.27 (from langchain_community)\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain_community)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain_community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
            "  Downloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.11.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langsmith<1.0.0,>=0.1.125 (from langchain_community)\n",
            "  Downloading langsmith-0.4.33-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading propcache-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
            "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain<2.0.0,>=0.3.27->langchain_community)\n",
            "  Downloading langchain_text_splitters-0.3.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain_community) (2.11.9)\n",
            "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=0.3.78->langchain_community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.78->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\n",
            "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain_community)\n",
            "  Downloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain_community)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain_community)\n",
            "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.8.3)\n",
            "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community)\n",
            "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.78->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.31-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.13.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.2-py3-none-any.whl (9.0 kB)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.78-py3-none-any.whl (449 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.4.33-py3-none-any.whl (387 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.3/387.3 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.11.0-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.6/48.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.43-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.11-py3-none-any.whl (33 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.11.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.4.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.5/221.5 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m146.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: zstandard, requests, python-dotenv, propcache, orjson, mypy-extensions, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed SQLAlchemy-2.0.43 aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.2.4 httpx-sse-0.4.2 jsonpatch-1.33 langchain-0.3.27 langchain-core-0.3.78 langchain-text-splitters-0.3.11 langchain_community-0.3.31 langsmith-0.4.33 marshmallow-3.26.1 mypy-extensions-1.1.0 orjson-3.11.3 propcache-0.4.0 pydantic-settings-2.11.0 python-dotenv-1.1.1 requests-2.32.5 requests-toolbelt-1.0.0 typing-inspect-0.9.0 yarl-1.22.0 zstandard-0.25.0\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
            "Downloading sentence_transformers-5.1.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.6/486.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-5.1.1\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Collecting groq\n",
            "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.3.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.78)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.56.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.9.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.3.8-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: groq, langchain-groq\n",
            "Successfully installed groq-0.32.0 langchain-groq-0.3.8\n"
          ]
        }
      ],
      "source": [
        "!pip install PyMuPDF -q\n",
        "!pip install langchain-openai\n",
        "!pip install langchain_community\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain langchain-community sentence-transformers faiss-cpu PyMuPDF\n",
        "!pip install langchain langchain-community sentence-transformers faiss-cpu PyMuPDF groq langchain-groq python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN4vFzm6pgAl"
      },
      "source": [
        "## Making the required files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vJb9Y6VMWSR",
        "outputId": "fb7a34d2-add9-4797-97af-08a48bd3d5f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory 'raw_data' created.\n",
            "Please upload your PDF and JSON files to the 'raw_data' folder in the file browser on the left.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define the source directory for your raw files\n",
        "source_directory = \"raw_data\"\n",
        "\n",
        "# Define the destination directory for the converted text files\n",
        "destination_directory = \"processed_data\"\n",
        "\n",
        "# Create the directories\n",
        "os.makedirs(source_directory, exist_ok=True)\n",
        "os.makedirs(destination_directory, exist_ok=True)\n",
        "\n",
        "print(f\"Directory '{source_directory}' created.\")\n",
        "print(f\"Please upload your PDF and JSON files to the '{source_directory}' folder in the file browser on the left.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE2wDnn1p11n"
      },
      "source": [
        "## Converting Raw Data into Text format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZRBcfzOMe1s",
        "outputId": "7fdb047a-572b-4761-e7eb-73a3bc1931f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Converting PDF files to text... ---\n",
            "Successfully converted 'mql5book.pdf' to 'mql5book.txt'.\n",
            "Successfully converted 'neuronetworksbook.pdf' to 'neuronetworksbook.txt'.\n",
            "Successfully converted 'mql5.pdf' to 'mql5.txt'.\n",
            "\n",
            "--- Converting JSON files to text... ---\n",
            "\n",
            "Conversion complete! All processed documents are now in the 'processed_data' directory.\n",
            "You can now use these text files for your next processing steps.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "# --- Function to Convert PDFs to Text ---\n",
        "def convert_pdfs_to_txt(source_dir: str, dest_dir: str):\n",
        "    \"\"\"Converts all PDF files in a source directory to text files.\"\"\"\n",
        "    print(\"--- Converting PDF files to text... ---\")\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Source directory '{source_dir}' not found.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.lower().endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(source_dir, filename)\n",
        "            txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "            txt_path = os.path.join(dest_dir, txt_filename)\n",
        "\n",
        "            try:\n",
        "                doc = fitz.open(pdf_path)\n",
        "                text_content = \"\"\n",
        "                for page in doc:\n",
        "                    text_content += page.get_text()\n",
        "\n",
        "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text_content)\n",
        "\n",
        "                print(f\"Successfully converted '{filename}' to '{txt_filename}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting '{filename}': {e}\")\n",
        "\n",
        "\n",
        "# --- Function to Convert JSON to Text ---\n",
        "def convert_json_to_txt(source_dir: str, dest_dir: str):\n",
        "    \"\"\"Converts all JSON files with Q&A pairs to a single text file.\"\"\"\n",
        "    print(\"\\n--- Converting JSON files to text... ---\")\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Source directory '{source_dir}' not found.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        if filename.lower().endswith(\".json\"):\n",
        "            json_path = os.path.join(source_dir, filename)\n",
        "            txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "            txt_path = os.path.join(dest_dir, txt_filename)\n",
        "\n",
        "            try:\n",
        "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    data = json.load(f)\n",
        "\n",
        "                text_content = \"\"\n",
        "                # Check if data is a list of dictionaries (Q&A format)\n",
        "                if isinstance(data, list):\n",
        "                    for entry in data:\n",
        "                        if \"question\" in entry and \"answer\" in entry:\n",
        "                            text_content += f\"Question: {entry['question']}\\nAnswer: {entry['answer']}\\n\\n\"\n",
        "                else:\n",
        "                    print(f\"Warning: JSON file '{filename}' is not in the expected Q&A list format. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "                with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(text_content)\n",
        "\n",
        "                print(f\"Successfully converted '{filename}' to '{txt_filename}'.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error converting '{filename}': {e}\")\n",
        "\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "\n",
        "# Define directory paths (using the variables from the previous cell)\n",
        "source_data_dir = \"raw_data\"\n",
        "dest_data_dir = \"processed_data\"\n",
        "\n",
        "# Run the conversion functions\n",
        "convert_pdfs_to_txt(source_data_dir, dest_data_dir)\n",
        "convert_json_to_txt(source_data_dir, dest_data_dir)\n",
        "\n",
        "print(f\"\\nConversion complete! All processed documents are now in the '{dest_data_dir}' directory.\")\n",
        "print(\"You can now use these text files for your next processing steps.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpD-D570qGbV"
      },
      "source": [
        "## Storing data in Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tDcCRrzMlJZ",
        "outputId": "31e8fd35-10aa-4472-943e-f01b9132d522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the FAISS index creation process...\n",
            "Loaded and processed 3 content blocks. Splitting...\n",
            "Split into 15230 chunks. Generating embeddings with OpenAI...\n",
            "FAISS index created successfully!\n",
            "Index saved at: data/faiss_index\n"
          ]
        }
      ],
      "source": [
        "# FILE: create_faiss_index.py (Final Corrected Version)\n",
        "# DESCRIPTION: Uses RecursiveCharacterTextSplitter to handle any document format.\n",
        "\n",
        "import os\n",
        "import re\n",
        "# --- CHANGED: Import the more robust RecursiveCharacterTextSplitter ---\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from dotenv import load_dotenv\n",
        "from google.colab import userdata # Good practice to import this for Colab\n",
        "\n",
        "# Define paths\n",
        "DOCS_PATH = \"processed_data\"\n",
        "FAISS_INDEX_PATH = \"data/faiss_index\"\n",
        "\n",
        "\n",
        "def setup_api_key():\n",
        "    \"\"\"Sets up the OpenAI API key from Colab secrets.\"\"\"\n",
        "    # load_dotenv() # <--- REMOVE THIS LINE. It's not needed.\n",
        "\n",
        "    # os.getenv is one way, but userdata.get is the more modern Colab way\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found. Make sure it's set in Colab's Secrets manager.\")\n",
        "    os.environ['OPENAI_API_KEY'] = api_key\n",
        "\n",
        "def separate_code_and_text(content: str):\n",
        "    \"\"\"\n",
        "    Separates text content into natural language and code blocks\n",
        "    using markdown-style ``` fences.\n",
        "    \"\"\"\n",
        "    code_pattern = re.compile(r\"```(.*?)```\", re.DOTALL)\n",
        "    code_blocks = [match.group(1).strip() for match in code_pattern.finditer(content)]\n",
        "    text_content = code_pattern.sub(\"\", content).strip()\n",
        "    return text_content, code_blocks\n",
        "\n",
        "def create_faiss_index():\n",
        "    \"\"\"\n",
        "    Processes documents, creates embeddings with OpenAI, and saves a FAISS index.\n",
        "    \"\"\"\n",
        "    print(\"Starting the FAISS index creation process...\")\n",
        "    setup_api_key()\n",
        "\n",
        "    all_docs = []\n",
        "    if not os.path.exists(DOCS_PATH):\n",
        "        print(f\"Error: Directory '{DOCS_PATH}' not found.\")\n",
        "        return\n",
        "\n",
        "    # 1. Load each file and process its content\n",
        "    for filename in os.listdir(DOCS_PATH):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(DOCS_PATH, filename)\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            text, code = separate_code_and_text(content)\n",
        "\n",
        "            if text:\n",
        "                all_docs.append(Document(page_content=text, metadata={\"source\": filename, \"type\": \"text\"}))\n",
        "            for i, code_block in enumerate(code):\n",
        "                if code_block:\n",
        "                    code_source = f\"{filename}_code_{i+1}\"\n",
        "                    all_docs.append(Document(page_content=code_block, metadata={\"source\": code_source, \"type\": \"code\"}))\n",
        "\n",
        "    if not all_docs:\n",
        "        print(f\"No processable content found in '{DOCS_PATH}'.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Loaded and processed {len(all_docs)} content blocks. Splitting...\")\n",
        "\n",
        "    # 2. Split documents into smaller chunks\n",
        "    # --- CHANGED: Switched to RecursiveCharacterTextSplitter ---\n",
        "    # This will correctly split the document into ~1000 character chunks.\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    split_docs = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    print(f\"Split into {len(split_docs)} chunks. Generating embeddings with OpenAI...\")\n",
        "\n",
        "    # 3. Create embeddings (keeping the batching fix from before)\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", chunk_size=100)\n",
        "\n",
        "    # 4. Create the FAISS index from documents\n",
        "    db = FAISS.from_documents(split_docs, embeddings)\n",
        "    db.save_local(FAISS_INDEX_PATH)\n",
        "\n",
        "    print(\"FAISS index created successfully!\")\n",
        "    print(f\"Index saved at: {FAISS_INDEX_PATH}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(os.path.dirname(FAISS_INDEX_PATH), exist_ok=True)\n",
        "    create_faiss_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LliIwfqU6K"
      },
      "source": [
        "## RAG model for answering users' queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDgfWZgtsxab",
        "outputId": "6a68a244-c653-418d-9f97-d3b969a493a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- API Key is set. ---\n",
            "--- Setting up RAG Components ---\n",
            "Loading FAISS index from: data/faiss_index...\n",
            "--- RAG Components setup complete. ---\n",
            "\n",
            "==================================================\n",
            "Ask a question (or type 'exit' to quit): mt5 function that takes trades using RSI\n",
            "Retrieving documents...\n",
            "r.result.order = cnv[array[3]];\n",
            "            r.result.volume = array[4];\n",
            "            r.result.price = array[5];\n",
            "            PrintFormat(\"Got Req=%d at %d ms\",\n",
            "               r.result.request_id, GetTickCount() - start);\n",
            "            Print(TU::StringOf(r.result));\n",
            "            return true;\n",
            "         }\n",
            "      }\n",
            "   }\n",
            "   Print(\"Timeout for: \");\n",
            "   Print(TU::StringOf(r));\n",
            "   return false;\n",
            "}\n",
            "Now that we have this function, let's write a trading algorithm in an asynchronous-synchronous style:\n",
            "as a direct sequence of steps, each of which waits for the previous one to be ready due to notifications\n",
            "from the parallel indicator program while remaining inside one function.\n",
            "Part 6. Trading automation\n",
            "1382\n",
            "6.4 Creating Expert Advisors\n",
            "void OnTimer()\n",
            "{\n",
            "   EventKillTimer();\n",
            "   \n",
            "   MqlTradeRequestSync::AsyncEnabled = true;\n",
            "   \n",
            "   MqlTradeRequestSync request;\n",
            "   request.magic = Magic;\n",
            "   request.deviation = Deviation;\n",
            "   \n",
            "   const double volume = Volume == 0 ?\n",
            "\n",
            "---\n",
            "\n",
            "trading environment. The function is similar to OrderCalcProfit.\n",
            "Example:\n",
            "import MetaTrader5 as mt5\n",
            "# display data on the MetaTrader 5 package\n",
            "print(\"MetaTrader5 package author: \",mt5.__author__)\n",
            "print(\"MetaTrader5 package version: \",mt5.__version__)\n",
            "# establish connection to MetaTrader 5 terminal\n",
            "if not mt5.initialize():\n",
            "    print(\"initialize() failed, error code =\",mt5.last_error())\n",
            "Python Integration\n",
            "© 2000-2025, MetaQuotes Ltd.\n",
            "3170\n",
            "    quit()\n",
            "# get account currency\n",
            "account_currency=mt5.account_info().currency\n",
            "print(\"Account currency:\",account_currency)\n",
            "# arrange the symbol list\n",
            "symbols = (\"EURUSD\",\"GBPUSD\",\"USDJPY\")\n",
            "print(\"Symbols to check margin:\", symbols)\n",
            "# estimate profit for buying and selling\n",
            "lot=1.0\n",
            "distance=300\n",
            "for symbol in symbols:\n",
            "    symbol_info=mt5.symbol_info(symbol)\n",
            "    if symbol_info is None:\n",
            "        print(symbol,\"not found, skipped\")\n",
            "        continue\n",
            "    if not symbol_info.visible:\n",
            "        print(symbol, \"is not visible, trying to switch on\")\n",
            "\n",
            "---\n",
            "\n",
            "Experts   automated trading is enabled\n",
            "   Trade request verification completed successfully\n",
            "   Retcode:      0\n",
            "   Balance:      10779.50 USD\n",
            "   Equity:       10779.50 USD\n",
            "   Profit:       0.00 USD\n",
            "   Margin:       1104.79 USD\n",
            "   Margin free:  9674.71 USD\n",
            "   Margin level: 975.71 %\n",
            "   Comment:      Done\n",
            "   OrderSend error 4756\n",
            "   Trade request result: retcode=10018, deal=0, order=0\n",
            "   \n",
            "   check on the open market:\n",
            "   Trade request verification completed successfully\n",
            "   Retcode:      0\n",
            "   Balance:      10779.50 USD\n",
            "   Equity:       10779.50 USD\n",
            "   Profit:       0.00 USD\n",
            "   Margin:       110.46 USD\n",
            "   Margin free:  10669.04 USD\n",
            "   Margin level: 9758.74 %\n",
            "   Comment:      Done\n",
            "   Trade request result: retcode=10009, deal=2777010968, order=2802818813\n",
            "   */\n",
            "  }\n",
            "Trade Functions\n",
            "© 2000-2025, MetaQuotes Ltd.\n",
            "2262\n",
            "//+------------------------------------------------------------------+\n",
            "//| Prepare parameters for a trade request                           |\n",
            "Generating answer...\n",
            "\n",
            "--- Answer ---\n",
            "To create a trading function using the Relative Strength Index (RSI) in MetaTrader 5 (MT5), you need to integrate the RSI indicator into your trading strategy and use it to make buy or sell decisions. Here's a step-by-step explanation of how you can achieve this:\n",
            "\n",
            "1. **Initialize the MT5 Environment**: Before you can execute trades, you need to establish a connection with the MT5 terminal. This involves importing the MetaTrader5 package and initializing it. If the initialization fails, you should handle the error appropriately.\n",
            "\n",
            "2. **Retrieve Account Information**: It's important to know the account details such as the currency, balance, and margin levels. This information helps in managing risk and ensuring that you have sufficient funds to execute trades.\n",
            "\n",
            "3. **Define the RSI Indicator**: The RSI is a momentum oscillator that measures the speed and change of price movements. It ranges from 0 to 100 and is typically used to identify overbought or oversold conditions. An RSI above 70 may indicate that a security is overbought, while an RSI below 30 may indicate that it is oversold.\n",
            "\n",
            "4. **Set Up the Trading Logic**: \n",
            "   - **Buy Signal**: If the RSI value falls below 30, it might be a signal to buy, anticipating that the price will rise.\n",
            "   - **Sell Signal**: Conversely, if the RSI rises above 70, it might be a signal to sell, anticipating that the price will fall.\n",
            "\n",
            "5. **Execute Trades**: Use the `mt5.order_send()` function to place buy or sell orders based on the RSI signals. Ensure that you specify the correct parameters such as the symbol, order type, volume, and price.\n",
            "\n",
            "6. **Error Handling and Logging**: After sending a trade request, check the result for any errors. Log the results for future reference and debugging. This includes checking the return code and any comments provided by the MT5 server.\n",
            "\n",
            "7. **Example Code**:\n",
            "   ```python\n",
            "   import MetaTrader5 as mt5\n",
            "\n",
            "   # Initialize MT5\n",
            "   if not mt5.initialize():\n",
            "       print(\"initialize() failed, error code =\", mt5.last_error())\n",
            "       quit()\n",
            "\n",
            "   # Define RSI parameters\n",
            "   rsi_period = 14\n",
            "   rsi_overbought = 70\n",
            "   rsi_oversold = 30\n",
            "\n",
            "   # Define trading parameters\n",
            "   symbol = \"EURUSD\"\n",
            "   lot = 1.0\n",
            "\n",
            "   # Get RSI value\n",
            "   rates = mt5.copy_rates_from_pos(symbol, mt5.TIMEFRAME_H1, 0, rsi_period + 1)\n",
            "   close_prices = [rate.close for rate in rates]\n",
            "   rsi_value = calculate_rsi(close_prices, rsi_period)\n",
            "\n",
            "   # Trading logic\n",
            "   if rsi_value < rsi_oversold:\n",
            "       # Buy order\n",
            "       request = {\n",
            "           \"action\": mt5.TRADE_ACTION_DEAL,\n",
            "           \"symbol\": symbol,\n",
            "           \"volume\": lot,\n",
            "           \"type\": mt5.ORDER_TYPE_BUY,\n",
            "           \"price\": mt5.symbol_info_tick(symbol).ask,\n",
            "           \"deviation\": 20,\n",
            "           \"magic\": 234000,\n",
            "           \"comment\": \"RSI buy\",\n",
            "           \"type_time\": mt5.ORDER_TIME_GTC,\n",
            "           \"type_filling\": mt5.ORDER_FILLING_RETURN,\n",
            "       }\n",
            "       result = mt5.order_send(request)\n",
            "       print(\"Buy order result:\", result)\n",
            "\n",
            "   elif rsi_value > rsi_overbought:\n",
            "       # Sell order\n",
            "       request = {\n",
            "           \"action\": mt5.TRADE_ACTION_DEAL,\n",
            "           \"symbol\": symbol,\n",
            "           \"volume\": lot,\n",
            "           \"type\": mt5.ORDER_TYPE_SELL,\n",
            "           \"price\": mt5.symbol_info_tick(symbol).bid,\n",
            "           \"deviation\": 20,\n",
            "           \"magic\": 234000,\n",
            "           \"comment\": \"RSI sell\",\n",
            "           \"type_time\": mt5.ORDER_TIME_GTC,\n",
            "           \"type_filling\": mt5.ORDER_FILLING_RETURN,\n",
            "       }\n",
            "       result = mt5.order_send(request)\n",
            "       print(\"Sell order result:\", result)\n",
            "\n",
            "   # Shutdown MT5\n",
            "   mt5.shutdown()\n",
            "   ```\n",
            "\n",
            "This code provides a basic framework for trading using the RSI indicator in MT5. You can expand upon it by adding more sophisticated risk management and trade management strategies.\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# FILE: final_rag_app.py\n",
        "# This script achieves your goal of \"Document-First, then AI Knowledge\" using only one API key.\n",
        "\n",
        "import os\n",
        "import traceback\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "CONFIG = {\n",
        "    \"FAISS_INDEX_PATH\": \"data/faiss_index\",\n",
        "    \"EMBEDDING_MODEL_NAME\": \"text-embedding-3-small\",\n",
        "    \"LLM_MODEL_NAME\": \"gpt-4o\",\n",
        "    \"SEARCH_K\": 3\n",
        "}\n",
        "\n",
        "# --- API KEY SETUP ---\n",
        "def setup_api_key():\n",
        "    \"\"\"Sets up the OpenAI API key from a local .env file.\"\"\"\n",
        "    load_dotenv()\n",
        "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "        raise ValueError(\"OPENAI_API_KEY not found.\")\n",
        "    print(\"--- API Key is set. ---\")\n",
        "\n",
        "# --- RAG SETUP ---\n",
        "def setup_rag_components():\n",
        "    \"\"\"Initializes the main components for the RAG system.\"\"\"\n",
        "    print(\"--- Setting up RAG Components ---\")\n",
        "    llm = ChatOpenAI(model_name=CONFIG['LLM_MODEL_NAME'], temperature=0.1)\n",
        "    embeddings = OpenAIEmbeddings(model=CONFIG['EMBEDDING_MODEL_NAME'])\n",
        "\n",
        "    if not os.path.exists(CONFIG['FAISS_INDEX_PATH']):\n",
        "        raise FileNotFoundError(f\"FAISS index not found at '{CONFIG['FAISS_INDEX_PATH']}'. Please run the index creation script first.\")\n",
        "\n",
        "    print(f\"Loading FAISS index from: {CONFIG['FAISS_INDEX_PATH']}...\")\n",
        "    db = FAISS.load_local(\n",
        "        CONFIG['FAISS_INDEX_PATH'], embeddings, allow_dangerous_deserialization=True\n",
        "    )\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": CONFIG['SEARCH_K']})\n",
        "    print(\"--- RAG Components setup complete. ---\")\n",
        "    return llm, retriever\n",
        "\n",
        "# --- MAIN LOOP ---\n",
        "# --- CORRECTED VERSION ---\n",
        "\n",
        "def main():\n",
        "    # Level 1 Indentation\n",
        "    try:\n",
        "        # Level 2 Indentation (all code inside 'try' is indented once more)\n",
        "        setup_api_key()\n",
        "        llm, retriever = setup_rag_components()\n",
        "\n",
        "        enriched_template = \"\"\"\n",
        "You are an expert technical assistant who provides comprehensive and easy-to-understand answers.\n",
        "\n",
        "Your main goal is to answer the user's QUESTION using the provided CONTEXT from their local document as the primary source of truth. After explaining the information from the CONTEXT, you must enrich it with your own general knowledge.\n",
        "\n",
        "Your instructions are:\n",
        "1.  First, carefully analyze the CONTEXT and extract the key information to directly answer the QUESTION.\n",
        "2.  Synthesize a complete answer by merging the information from the CONTEXT with your own broader knowledge.\n",
        "3.  Use your general knowledge to:\n",
        "    - Define any technical terms or jargon mentioned in the context.\n",
        "    - Explain the \"why\" behind the information in the context (e.g., why a certain piece of code is written that way).\n",
        "    - Provide additional relevant background or examples that make the topic easier to understand.\n",
        "4.  Structure your answer clearly. It should feel like a single, cohesive explanation, not two separate parts.\n",
        "5.  **Do not** mention the context directly in your answer (e.g., avoid phrases like \"According to the document...\").\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "COMPREHENSIVE ANSWER:\n",
        "\"\"\"\n",
        "        final_prompt = PromptTemplate.from_template(enriched_template)\n",
        "\n",
        "        while True:\n",
        "            print(\"\\n\" + \"=\" * 50)\n",
        "            user_query = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "            if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "                print(\"Exiting...\")\n",
        "                break\n",
        "            if not user_query.strip():\n",
        "                continue\n",
        "\n",
        "            print(\"Retrieving documents...\")\n",
        "            retrieved_docs = retriever.invoke(user_query)\n",
        "            context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "            print(context_text)\n",
        "\n",
        "            print(\"Generating answer...\")\n",
        "\n",
        "            final_chain = final_prompt | llm | StrOutputParser()\n",
        "            response = final_chain.invoke({\n",
        "                \"context\": context_text,\n",
        "                \"question\": user_query\n",
        "            })\n",
        "\n",
        "            print(\"\\n--- Answer ---\")\n",
        "            print(response)\n",
        "\n",
        "    # Level 1 Indentation (perfectly aligned with 'try')\n",
        "    except Exception as e:\n",
        "        # Level 2 Indentation (code inside 'except' is indented once more)\n",
        "        print(\"\\nAn unexpected error occurred:\")\n",
        "        print(f\"{type(e).__name__}: {e}\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}